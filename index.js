var data = [
{
url: "http://shidetake.com/geoapi/",
title: "Yahoo!ジオコーダAPIで住所から経度緯度を取得",
content: "[Rubyを使う。 アプリケーションID登録 まずはAPIを使うための登録。 アプリケーション登録ページから登録できる。 デフォルトの設定のまま登録でOK。 アプリケーションIDを登録するに記入例があるので、 ちゃんとした登録をする場合は参考になる。 お試しアクセス 登録時に表示されるClient IDを使って、以下のURLにブラウザからアクセスする。 CLIENT_IDという部分を自分のClient IDに置き換えること。 東京都の各区の情報が表示されれば成功。 https://map.yahooapis.jp/geocode/V1/geoCoder?appid=CLIENT_ID&amp;amp;query=東京都 Rubyでアクセス 上と同じく、CLIENT_IDという部分を自分のClient IDに置き換えること。 require &#39;json&#39; require &#39;open-uri&#39; CLIENT_ID = &#39;CLIENT_ID&#39; class Geocoder def exec base_url = &#39;https://map.yahooapis.jp/geocode/V1/geoCoder&#39; params = { &#39;appid&#39; =&amp;gt; CLIENT_ID, &#39;query&#39; =&amp;gt; &#39;東京都&#39;, &#39;results&#39; =&amp;gt; &#39;1&#39;, &#39;output&#39; =&amp;gt; &#39;json&#39;, } url = base_url &#43; &#39;?&#39; &#43; URI.encode_www_form(params) res = JSON.parse(open(url).read) lon, lat = res[&#39;Feature&#39;][0][&#39;Geometry&#39;][&#39;Coordinates&#39;].split(&#39;,&#39;) puts &amp;quot;経度: #{lon}&amp;quot; puts &amp;quot;緯度: #{lat}&amp;quot; end end geocoder = Geocoder.new geocoder.exec]"
}
,{
url: "http://shidetake.com/aws_cli/",
title: "AWSのRDSインスタンスをCLIで操作",
content: "[AWSのRDSは使っていないときは停止しないともったいない。 とは言え、毎回ブラウザでログインして操作するのはかったるいので、コマンドラインで操作できるようにした。 awsコマンドは既に導入しているという前提。 インスタンス名の取得 操作するインスタンスの名前を取得する。 自分で名前を付けているので、わざわざ取得しなくても知っていると思うが。 飛ばして次に進んでも良い。 $ aws rds describe-db-instances とすると、ずらずらとインスタンスの情報が出てくる。 DBInstanceIdentifierという項目に名前がある。 停止 以下のコマンドで停止できる。 $ aws rds stop-db-instance --db-instance-identifier $instance 開始 以下のコマンドで開始できる。 $ aws rds start-db-instance --db-instance-identifier $instance GitHubに簡単に操作するためのシェルスクリプトを置いたので、参考にして欲しい。 インスタンスの名前と状態のリスト表示、全インスタンスの開始、全インスタンスの停止ができる（はず）。 手元の環境だと1インスタンスしかないので、複数インスタンスがある場合のテストはしていない。 参考 RDS の停止機能を使ってコストを半分まで削減してみた]"
}
,{
url: "http://shidetake.com/big_image/",
title: "画像を任意のサイズ（容量）に変換する",
content: "[テストのために容量の大きい画像が必要になったが、 案外見つからなくて、結局ImageMagickで適当な画像を変換して作ることにしたので、そのやり方を記す。 iPhoneとかで取った写真だと2〜3MBくらい。 今回欲しかったのは5MB以上だったのでダメ。 前に一眼レフで撮ってもらった写真があったなと思い、Google Photoで探したら、数百KBくらいだった。 Google Photoが頑張って圧縮しちゃっていたようだ。 インターネットで適当に探しても、5MB以上となるとなかなか出てこない。 1GBオーバーの天体画像なら出てきたが、さすがにでかすぎた。 ということで、ImageMagickに頼ることにした。 convert input.jpg -resize 2048x2048 output.bmp こんな感じ。ビットマップなので、2048x2048の画像でだいたい12MB。 完全に任意の容量にするにはちょっと計算が必要だけど、ざっくりでいいならこれで作れる。 ビットマップにしているのは、無圧縮のほうが計算しやすいため。 RGBそれぞれ1バイトずつなので、1画素あたり3MBになる。 1024x1024サイズだと、画素数がちょうど1Mなので、3MB。5MBには少し足りないので、 縦横倍にして4倍の12MBの画像になるというわけ。]"
}
,{
url: "http://shidetake.com/heroku_domains/",
title: "Herokuの公開URLをコマンドで確認する",
content: "[最近、Ruby on Railsチュートリアルを進めているんだけど、 その中でHerokuを使うことになる。で、途中まで進めて、別の日に続きをやろうとすると、作っているWebサービスのURLがわからなくなり、 いちいちHerokuにログインして確認するという手間が発生する。 たぶんherokuコマンドにあるだろうと思って探したらあったので、備忘録的に残しておく。 $ heroku domains これだけ。]"
}
,{
url: "http://shidetake.com/alexa_node_red1/",
title: "AlexaとNode-REDを利用したスマートホームスキル作成時の注意",
content: "[最近、AlexaとNode-REDを連携してスマートホームスキルを作っている。 前回のApple TVを操作するスクリプトもその一環だったのだが、うまく動かなかったのでその対処法を書く。 AlexaとNode-REDの連携 Amazon Echoとラズパイで、音声で照明をon/offするという記事を参考にした。 丁寧に書いてあって、補足する必要が全く無いので、連携についてはこちらの記事を読むといい。 execにpyatvを設定 こんな感じで、「Alexa, テレビつけて」でmenuボタンを押すようにしたんだけど、うまく動かない。 よく見ると、atvremoteコマンドが見つからないというエラーが出ていた。 systemdで起動しているサービスは、ユーザーの環境変数を読み込まないらしい。 systemdについては前にも書いたが、絶対パスで書かないといけないとか、いろいろ作法があって面倒くさい。 systemdの設定変更 pyenvを使って導入したソフトをsystemdで使う場合の注意点はpyenvの公式に記載があったのでこれを参考にした。 まず以下のコマンドでNode-REDのサービスファイルの場所を調べる。 sudo systemctl status nodered たぶん /lib/systemd/system/nodered.serviceにある。 これを書き換える。 Environment=&amp;quot;PI_NODE_OPTIONS=--max_old_space_size=256&amp;quot; Environment=&amp;quot;PATH=/home/user_name/.pyenv/shims:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&amp;quot; PI_NODE_OPTIONSという環境変数を設定しているところの下に、PATHの設定を追加する。 デフォルトでは、/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binがPATHに設定されているようなので、 そこに追加する形。 ちなみに環境変数の展開もできないので、こんな書き方はNG。ホントに面倒くさい。 Environment=&amp;quot;PATH=/home/user_name/.pyenv/shims:$PATH&amp;quot;]"
}
,{
url: "http://shidetake.com/pyatv/",
title: "Apple TVをRaspberry Piで操作",
content: "[スマートホーム化のために、いろんな家電をRaspberry Piで動かせるようにしている。 テレビはほとんどApple TVのために存在しているので、これを動かしてみた。 使ったのは、pyatvというツール。 postlund/pyatvにインストール方法から使い方まで書いてあるが、 一部わかりにくいところがあったので、そこだけ説明する。 初期設定 pip install pyatvしたあと、ペアリングが必要。 iTunesのホームシェアリングを有効にしている場合はいらないようだけど、 無効の場合は、 atvremote pair して、Apple TV側で、設定 &amp;gt; リモコンとデバイス &amp;gt; Remote Appとデバイス と進み、pyatvを選択して、pinに1234と入力すればよい。 動作確認 atvremote -a menu これでMENUボタンを押したときと同じように動けばOK。]"
}
,{
url: "http://shidetake.com/gmail2line/",
title: "Gmailに重要なメールが届いたらLINEに通知する",
content: "[最近はめっきりメールを使わなくなった。iOSもメール機能なんかどうでもいいと思っているのか、 いまいち使い勝手が悪い。Gmailのプッシュ通知に非対応ってどうなの。 それでもAmazonの注文メールやら、航空券の予約完了メールやら、色々な場面で送られてくるので、嫌々ながら使っている。 中には緊急性のあるメールもあって、その筆頭が銀行引き落とし失敗メール。残高が足りずに引き落とされなかったときに送られてくる。 早めに気づいて入金できれば、再度引き落としのタイミングがあって、なんとかカードを止められずにすむ。 今回は、引き落としに失敗した時にLINE通知してくれるシステムを構築する。 参考サイト 先に白状しておくと、この記事は Google Apps ScriptでGmailの特定のメールを受信したらLINEと連携して通知するという記事のほとんどパクリです。 構成 Google Apps ScriptとLINE Messaging APIを組み合わせて使う。 IFTTTあたりを使うともっと簡単に作れるが、ポーリング間隔が数時間と長すぎるので今回はNG。 Google Apps Scriptだと1分間隔でポーリング可能。 事前準備 LINE Messaging APIを使えるようにしておく(別記事参照)。 Gmailにtreatedラベルを作る 1度処理したメールを再度処理しないようにするためのラベル Google Apps Script 以下のようなスクリプトを作る。 GmailからQUERYにマッチするメールを取得して、メールの数だけLINEにメッセージを送る。 処理したメールはtreatedラベルを付けて、再度処理しないようにしている。（そのためにQUERYで-label:treatedしている） var CHANNEL_ACCESS_TOKEN = &#39;YOUR_CHANNEL_ACCESS_TOKEN&#39;; var LINE_USER_ID = &#39;YOUR_LINE_USER_ID&#39;; var QUERY = &#39;-label:treated subject:未済&#39;; function main() { var threads = GmailApp.search(QUERY, 0, 10); var messages = GmailApp.getMessagesForThreads(threads); for (var i in messages) { for (var j in messages[i]) { line_push(&#39;MUFJからの引き落とし失敗してるよ&#39;); } } // add label:treated var label_treated = GmailApp.getUserLabelByName(&#39;treated&#39;); for (var i in threads) { threads[i].addLabel(label_treated); } } // LINEにプッシュ通知する function line_push(message) { var postData = { &amp;quot;to&amp;quot;: LINE_USER_ID, &amp;quot;messages&amp;quot; : [ { &amp;quot;type&amp;quot; : &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot; : message } ] }; var options = { &amp;quot;method&amp;quot; : &amp;quot;post&amp;quot;, &amp;quot;headers&amp;quot; : { &amp;quot;Content-Type&amp;quot; : &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot; : &amp;quot;Bearer &amp;quot; &#43; CHANNEL_ACCESS_TOKEN }, &amp;quot;payload&amp;quot; : JSON.stringify(postData) }; UrlFetchApp.fetch(&amp;quot;https://api.line.me/v2/bot/message/push&amp;quot;, options); } トリガの設定 Google Apps Scriptのスクリプト編集画面で、編集 &amp;gt; 現在のプロジェクトのトリガーを選択する。 ここで、main関数を任意の時間間隔で実行するように設定できる。 最短は1分なのでとりあえず1分にしておいた。 おまけ 読めばわかると思うが、今回の例は三菱東京UFJ銀行の口座からの引き落としに失敗したときのケース。 このメールが届くようにするには、三菱東京UFJダイレクトで設定する必要がある（設定したのはだいぶ前なので詳細は忘れた）。 で、みずほ銀行も使っているのでこっちも同じようにしようと思ったら、みずほは通知メールサービスが無いらしい。 結構古い情報だけど、このブログ記事によると、銀行によって通知サービスが無かったり、有料だったりする模様。 サブバンクをみずほから新生銀行あたりに乗り換えようかなと思った。]"
}
,{
url: "http://shidetake.com/line_feeding/",
title: "定期的なリマインドをLINE BOTにやってもらう",
content: "[自分だけで完結するような定期処理は、適当なアラームアプリあたりに設定すれば良いが、 他人にお願いしていることは、わざわざアラーム設定しておいてと頼むのも角が立つので、こちらからリマインドしたい。 でも面倒なので、LINE BOTに頼むことにした。 構成 LINE BOTでPUSH通知するとMakefileでcronを登録するを組み合わせるだけ。 詳細はそれぞれの記事を読んでもらうとして、ソースを記載する。 スクリプト #!/bin/sh curl -X POST \ -H &#39;Content-Type:application/json&#39; \ -H &#39;Authorization: Bearer {CHANNEL_ACCESS_TOKEN}&#39; \ -d &#39;{ &amp;quot;to&amp;quot;: &amp;quot;GROUPID&amp;quot;, &amp;quot;messages&amp;quot;:[ { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;エサあげた？&amp;quot; } ] }&#39; https://api.line.me/v2/bot/message/push ここで、CHANNEL_ACCESS_TOKENはLINE depelopersで取得するトークン。 GROUPIDは、LINEのグループを識別するためのID。 これの取得方法は前の記事には書いてなかったので説明する。 GROUP IDの取得 以下のスクリプトをGoogle Apps Scriptでウェブアプリケーションとして導入し、 LINE BOTのWebhook URLに紐付ける。 その状態で、LINEグループにBOT招待すれば、example@gmailにGROUP IDが通知される。 function doPost(e) { Logger.log(&#39;doPost&#39;) var events = JSON.parse(e.postData.contents).events; events.forEach (function(event) { if (event.type == &amp;quot;join&amp;quot;) { mailGroupId(event); } }); } function mailGroupId(e) { MailApp.sendEmail(&#39;example@gmail.com&#39;, &#39;groupId&#39;, e.source.groupId); } cron登録 以下のようなMakefileを作って、make installすればよい。 ここでは、毎日9時と18時に通知する。 install: crontab -l | grep line_feeding.sh || \ (crontab -l; echo &amp;quot;0 9,18 * * * /usr/local/bin/line_feeding.sh&amp;quot;) | crontab ユースケース 今回の例は、犬を預かってもらう人にエサをちゃんとあげてくれるようにリマインドするケース。 自分で聞くのは忘れそうだし、BOTに機械的に聞いてもらうことで角が立ちにくい（気がする）。 あとは、今回の例では個人に通知するのではなく、グループに通知する形にしている。 これにより、相手が実行してくれたかどうか聞けるという利点もある（相手がBOTに返信してくれる人なら）。]"
}
,{
url: "http://shidetake.com/make_cron/",
title: "Makefileでcronを登録する",
content: "[主に重複登録をしないようにするためのtipsを紹介する。 cron登録コマンド echo &amp;quot;0 9,18 * * * /usr/local/bin/example.sh&amp;quot; | crontab こんな感じ。とても簡単。 ただ、これは設定を上書きするという非常に危険なコマンド。 通常は以下のように使う。 (crontab -l; echo &amp;quot;0 9,18 * * * /usr/local/bin/example.sh&amp;quot;) | crontab Makefile化 install: (crontab -l; echo &amp;quot;0 9,18 * * * /usr/local/bin/example.sh&amp;quot;) | crontab そのまんま。これで、make installすればcron に登録される。 これの問題点は、重複登録してしまうこと。 以下のように書くことで、重複登録が防げる。 install: crontab -l | grep example.sh || \ (crontab -l; echo &amp;quot;0 9,18 * * * /usr/local/bin/example.sh&amp;quot;) | crontab ||でつなぐことで、最初のコマンドが失敗したときだけ、次のコマンドを実行するようになる。 ここでは、既に登録されているcronをgrepして、登録しようとしているスクリプトがあれば何もせず終了、 なければ登録する。 uninstall おまけ。uninstallは以下のように書けば良い。 uninstall: crontab -l | grep -v example.sh | crontab]"
}
,{
url: "http://shidetake.com/alexa_skill_1/",
title: "Alexa SkillsでHello worldする",
content: "[Amazon Echoが届いたので、早速スキルを作ってみることにした。 やはり最初はHello world。 構成 Amazon Echoにスキルをインストールして、声でスキルを呼び出す。 呼び出されたスキルは、登録されたAWS Lambdaの関数を呼び出して、処理を実行するという流れ。 手順 ざっくり以下のような手順が必要 AWS LambdaでAlexaに実行させる関数を作成 Amazon DevelopperにてAlexaスキルを作成してAWS Lambda関数に紐付ける Amazon echoにインストール AWS Lamda AWS登録して、ログインしたら、AWS Lamdaの設定画面に進む。 右上にオハイオとか地名が書かれているので、東京にしておく。 関数の作成ボタンを押して、設計図からalexa-skill-kit-sdk-factskillを選ぶ。 基本的な情報は以下のように設定した。 設定項目 設定 名前 alexaHello ロール テンプレートから新しいロールを作成 ロール名 alexaHello Lambda関数のコードは以下のようにした。 ちなみにこれは、alexa/skill-sample-nodejs-hello-worldに公開されているindex.jsから必要な部分だけを残した。 &#39;use strict&#39;; const Alexa = require(&amp;quot;alexa-sdk&amp;quot;); exports.handler = function(event, context, callback) { const alexa = Alexa.handler(event, context); alexa.registerHandlers(handlers); alexa.execute(); }; const handlers = { &#39;LaunchRequest&#39;: function () { this.emit(&#39;SayHello&#39;); }, &#39;HelloWorldIntent&#39;: function () { this.emit(&#39;SayHello&#39;); }, &#39;SayHello&#39;: function () { this.response.speak(&#39;Hello World!&#39;); this.emit(&#39;:responseReady&#39;); } }; 関数の作成ボタンを押して、次の画面に進む。 トリガーの設定画面が出るので、Alexa Skills Kitを選択して、保存すればAWS Lambda側の設定は完了。 右上にARNが表示されている。スキルが関数を呼び出す際に使うので控えておく。 Amazon Developper Amazon Developper Services and Technologiesにアクセスして、右上のサインインからアカウントを作成する。 スキル作成 アカウントを作ってログインしたら、amazon alexaのページに進み、左上のメニューから、 &amp;ldquo;Alexa Skills Kit (ASK) &amp;gt; 始めてみよう&amp;rdquo;を選ぶ。 この辺のドキュメントは後で読むとして、&amp;rdquo;スキル開発を始める&amp;rdquo;。 スキル情報 スキル情報は以下の通り 設定項目 設定 スキルの種類 カスタム対話モデル 言語 Japanese スキル名 hello world 呼び出し名 hello world この設定で保存して次へ。 対話モデル インテントスキーマ { &amp;quot;intents&amp;quot;: [ { &amp;quot;intent&amp;quot;: &amp;quot;HelloWorldIntent&amp;quot; } ] } このインテントはAWS Lambdaで作った関数のfunction名と紐付ける。 サンプル発話 HelloWorldIntent hello 設定 サービスエンドポイントのタイプにAWS LamdaのARNを指定して、 デフォルト欄に、AWS Lambdaで作った関数のARNを入力する。 arn:aws:lambda:ap-northeast-1:xxxxxxxxxxxx:function:alexaHelloみたいな形式。 テスト 次の画面でテストできる。画面中ほどにあるサービスシミュレーターで、先ほど入力したARNが選択されていることを確認して、 テキスト入力欄にhelloと書いてhello worldを呼び出す。 サービスレスポンス側にある聴くというボタンを押すと、レスポンスが聴ける。ここでエラーが出ていなければ、スキルからAWS Lambdaの呼び出しは成功。 公開情報 あとはインストールするための準備。Skills Beta Testingというテスト用のインストール機能があるのでこれを使う。 これを使うためには、画面左上にある各種設定を完成させてオールグリーンにする必要がある。 ここまで正しくできていれば、残りは&amp;rdquo;公開情報&amp;rdquo;と&amp;rdquo;プライバシーとコンプライアンス&amp;rdquo;だけのはず。 基本的には全て適当に埋めれば良い。 公開情報は以下のように書いた。 設定項目 設定 カテゴリー Assistants テストの手順 hello 国と地域 Amazonがスキルを配布するすべての国と地域 スキルの簡単な説明 hello スキルの詳細な説明 hello サンプルフレーズ hello アイコン用の画像登録も必要。サイズ指定があるのでちゃんと正しいサイズの画像を用意すること。 imagemagickがあれば以下のコマンドで作れる。 convert -resize 108x108 original_icon.jpg icon_108.jpg convert -resize 512x512 original_icon.jpg icon_512.jpg プライバシーとコンプライアンス 設定項目 設定 このスキルを使って何かを購入したり、実際にお金を支払うことができますか？ いいえ このスキルはユーザーの個人情報を収集しますか？ いいえ このスキルは13歳未満の子供を対象にしていますか？ いいえ 輸出コンプライアンス チェック このスキルは広告を含みますか？ いいえ これで保存を押すと、左側のSkills Beta Testingのステータスがアクティブになるので、テストの管理ボタンを押して設定画面に遷移する。 Skills Beta Testing テスターを追加ボタンから自分のメールアドレスを入力すると、テスター招待メールが届く。 2つリンクがあるが、2つ目のリンクが日本人向けのもの。 JP customers: To get started, follow this link: Enable Alexa skill &amp;quot;hello world&amp;quot; これをクリックして、スキルを有効化すれば使えるようになる。]"
}
,{
url: "http://shidetake.com/git_showtool/",
title: "git showの差分をvimdiffで見る",
content: "[以前、git diffをvimdiffで見る方法を紹介した。 今回はgit showをvimdiffで見る方法を紹介する。 git show あるコミットの詳細を表示するコマンド。diffも表示してくれる。 git diff程使う機会は少ないかもしれないが、個人的にはかなりよく使うコマンド。 コードレビューや不具合解析なんかで、過去のコミットでの修正内容を確認するのに便利なんだけど、 クラシックなdiffで表示されるのが気に入らなかったのでvimdiffで表示する方法を探していた。 エイリアス 結論から言うと、git showでvimdiffを使う方法はわからなかった。 環境変数GIT_EXTERNAL_DIFFにうまいこと設定できればできそうな気もするが、情報が少ない。 今回はエイリアスを使った方法にした。 git showと言いつつ、裏ではgit diffを使っている。実際、git showしたときのdiffはgit diffを使っているのだから問題ない。 [alias] showtool = &amp;quot;!sh -c &#39;git difftool &amp;quot;${0}&amp;quot;~ &amp;quot;${0}&amp;quot;&#39;&amp;quot; チルダを使って1世代前のコミットからの差分を見るような形にしている。 sh -cを使ってgitを呼び出すような冗長な書き方は、ハッシュやブランチ名を引数として渡すため。 このエイリアスだと引数がないと動かない。 オリジナルのgit showの場合、引数を指定しないときはHEADを指定したときの動作をするので、これと揃えるために以下のように変更した。 [alias] showtool = &amp;quot;!sh -c &#39;if [ &amp;quot;sh&amp;quot; == &amp;quot;${0}&amp;quot; ]; then REVISION=&amp;quot;HEAD&amp;quot;; else REVISION=&amp;quot;${0}&amp;quot;; fi; git difftool $REVISION~ $REVISION&#39;&amp;quot; おまけ 差分ファイルが大量にあると、全ファイルの差分を表示するので非常にめんどくさいことになる。 そこで、pecoを使って選択的に差分ファイルを見れるようにしている。これが完成形。 [alias] showtool = &amp;quot;!sh -c &#39;if [ &amp;quot;sh&amp;quot; == &amp;quot;${0}&amp;quot; ]; then REVISION=&amp;quot;HEAD&amp;quot;; else REVISION=&amp;quot;${0}&amp;quot;; fi;\ git log -1 --stat-width=800 $REVISION | grep \&amp;quot;|\&amp;quot; | awk \&amp;quot; {print \\$1}\&amp;quot; | peco | xargs -o git difftool $REVISION~ $REVISION&#39;&amp;quot; なかなかトリッキーなコマンドなので、解説はいつか余裕があるときに書くことにする。]"
}
,{
url: "http://shidetake.com/ebroad/",
title: "マンション付属の光回線が早くなった話",
content: "[自宅の光回線が遅くて、スピードテストを自動実行するスクリプトで現状把握してみるといった内容の記事を書いた。 その後の話。 光回線についての調査 マンション付属の光回線で、壁についているLANポートに繋ぐと何の手続きもせずに接続できるタイプのもの。 当時の資料には書いてあったかもしれないが、この時点ではどんな会社の回線なのか不明だった。 マンションの管理会社に電話すれば一発だが、平日昼間に電話するのが難しいので、ひとまず調べられる範囲で調べてみた。 まずはとりあえずtracerouteしてみた。 するといくつかローカルっぽいIPアドレスを経て（恐らくマンションのルーター）、ap.yournet.ne.jpなんちゃらみたいな所につながっていた。 このアドレスで検索してみると、どうやらfreebit系と呼ばれる悪名高いバックボーンであることがわかった。 ざっと調べたところ、freebit系プロバイダというのは、フリービット株式会社のYourNet ISP Networkサービスというものを利用しているISPのことを言うらしい。 プロバイダ機能を自前で持たないISPに対してその機能を提供するというサービスのようだ。 このあたりを調べている時に知ったのだが、 確認くん&#43;というウェブサイトにアクセスすると、tracerouteなどしなくても簡単にわかる。 ここにアクセスして、お使いのプロバイダーがap.yournet.ne.jpならばfreebit系。 プロバイダに確認 マンションの管理会社に聞くと、イーブロードというプロバイダが管理しているとのことだったので、 電話して状況を確認してみることにした。 週末に電話すると、サポートにはつながるけど、少し専門的な話になると対応できないようで、 平日に専門部署から折り返すとの返事。 平日の昼間は電話に出れない可能性が高いので、バックボーンをfreebitから変更することはできないか知りたいので結果を留守電に入れて欲しいと伝えた。 あっけない結末 その後、すぐに留守電が入ったが、折り返し電話して欲しいという内容だけだった。 留守電に内容を入れてくれよと脱力して、しばらく放置していると、いつの間にか回線速度が速くなった。 確認くん&#43;で見てみると、プロバイダがOCNになっていた。どうやらバックボーンを変えたらしい。 数年前も同じくらいの速度が出ていたので、理由は不明だが、ときどきバックボーンが変わるのだと思う。 電話が効いたのかどうか知らないが、速くなったのでこれ以上の追求はやめた。]"
}
,{
url: "http://shidetake.com/make_win/",
title: "WindowsでもLinuxでも動くMakefileを書く",
content: "[シェルのコマンドが異なるので、両方に対応するのは意外とめんどくさい。 今回はシンボリックリンクを作成するコマンドを例にして説明する。 WindowsのmakeはMinGWのものを想定している。 Linux専用 install: ln -s ~/dotfiles/.vimrc ~/.vimrc; シンボリックリンクを作るコマンドをそのまま書いただけ。 Windows専用 install: cmd.exe /C mklink $(HOME)\.vimrc $(HOME)\dotfiles\.vimrc Windowsのコマンドを使う場合は、cmd.exe /Cを接頭語のように付ける。 これで、mklinkコマンドが使えるようになる。 Linux用と比べて以下の点が異なっている sourceとtargetの順番が逆 パスのディレクトリ区切りがバックスラッシュ ホームディレクトリの指定がチルダではなく$(HOME) 環境変数HOMEをあらかじめ指定する必要があるかも 両対応版 愚直に書くと以下のようになる。 ifeq ($(OS),Windows_NT)でWindowsかどうかを分岐させている。 install: ifeq ($(OS),Windows_NT) ln -s ~/dotfiles/.vimrc ~/.vimrc; else cmd.exe /C mklink $(HOME)\.vimrc $(HOME)\dotfiles\.vimrc endif この書き方だと、シンボリックリンクを張るファイルが増えると、Linux用とWindows用のそれぞれに追記する必要がある。 以下のように書くとスマート。 install: make link SOURCE:=$(HOME)/dotfiles/.vimrc TARGET:=$(HOME)/.vimrc link: ifeq ($(OS),Windows_NT) cmd.exe /C mklink $(subst /,\,$(TARGET)) $(subst /,\,$(SOURCE)) else ln -s $(SOURCE) $(TARGET) endif linkというターゲットを用意して、関数のように使っている。 linkの中でOSを分岐させることで、それを呼び出す側を統一できるというわけ。 substというのはmakeの文字列置換関数。スラッシュをバックスラッシュに置き換えるために使っている。 おまけ ここまででほぼ完成。 最後に鬱陶しい出力を抑えるために、--no-print-directoryしたり、シンボリックリンクが既に存在している場合にはコマンドをスキップする仕組みを入れたものを載せる （解説は割愛）。 MAKEFLAGS &#43;= --no-print-directory install: make link SOURCE:=$(HOME)/dotfiles/.vimrc TARGET:=$(HOME)/.vimrc link: ifeq ($(OS),Windows_NT) @cmd.exe /C if not exist $(subst /,\,$(TARGET)) \ cmd.exe /C mklink $(subst /,\,$(TARGET)) $(subst /,\,$(SOURCE)) else @if [ ! -e $(TARGET) ]; then\ ln -s $(SOURCE) $(TARGET);\ fi endif]"
}
,{
url: "http://shidetake.com/git_difftool/",
title: "gitのdiffをvimdiffで見る",
content: "[少し前まで、gitのdiffにvimdiffを使う場合、git_diff_wrapperなるファイルを用意して、 .gitconfigの[diff]セクションにexternal = git_diff_wrapperとするのが主流だった（と思っている）。 今、各種dotfileを見直している中で、このダサいやり方を改善できないか調べたところ、 今の主流はdifftoolを使うやり方のようなので、乗り換えた。 .gitconfig まず早速.gitconfigを示す。 [diff] tool = vimdiff [difftool] prompt = false これでOK。ただし、difftoolを使うのでコマンドが変わる。 diffのexternalを変える場合は、あくまでgit diffが使うツールを変更するという意味になるのだが、 diffのtoolオプションを変える場合は、git difftoolが使うツールを指定するという意味になる。 alias化 git_diff_wrapper手法ではgit diffでよかったのに、今回の方法ではgit difftoolと打たないといけないはイマイチなので、 エイリアスを用意する。 [diff] tool = vimdiff [difftool] prompt = false [alias] dt = difftool これでOK。本当はdiff = difftoolとしたかったが、もともと用意されているコマンドを上書きするようなエイリアスは作れないらしい。 リードオンリー解除 さて、これで今までとだいたい同じになったが、カレントのファイルがリードオンリーで開かれるのだけが不満。 diff見ながら手直しするというのはよくあるので、毎回:set noroなんて打ってられない。 なぜか以下のようにしたら解決した。これが完成形。 [diff] tool = vimdiff [difftool] prompt = false [difftool &amp;quot;vimdiff&amp;quot;] cmd = vimdiff $LOCAL $REMOTE [alias] dt = difftool [difftool &amp;quot;vimdiff&amp;quot;]セクションでcmdオプションを指定することで、difftoolで開くコマンドを指定できる。 ここに編集可能な状態で開くオプションを指定すればいいかなと思っていたが、特になんの指定もなくできた。 cmdオプションのデフォルトがどうなってるか知らないけど、リードオンリーな状態で開くようなオプションが指定されているのかな。 関連ポスト git showの差分をvimdiffで見る]"
}
,{
url: "http://shidetake.com/speedtest/",
title: "スピードテストを定期的に実行して回線速度の推移を見る",
content: "[自宅の回線速度が遅いので、なんとか改善できないか考えている。 まずは、現状把握ということで、スピードテストを実施した。 常に遅いのか、時間によって遅いのかなどがわかれば改善の手がかりになるかもしれないと考え、 cron定期的に実行してくれるようにした。 Raspberry Piで動作する。 実行結果 まずは作成したスクリプトで取得したデータを載せる。 csv形式で左からping値 [ms]、Download [Mbit/s], Upload [Mbit/s], 実行日時。 167.908,2.23,5.37,2017/10/16 00:00:39 129.306,7.82,5.82,2017/10/16 01:00:41 88.187,18.31,19.27,2017/10/16 02:00:36 87.879,24.88,21.74,2017/10/16 03:00:36 156.129,26.47,16.25,2017/10/16 04:00:41 75.103,25.61,21.42,2017/10/16 05:00:36 94.37,26.99,21.17,2017/10/16 06:00:40 76.879,24.55,21.35,2017/10/16 07:00:38 71.107,24.72,21.67,2017/10/16 08:01:39 254.614,19.98,16.24,2017/10/16 09:00:37 81.341,23.46,18.63,2017/10/16 10:00:37 89.473,23.30,17.17,2017/10/16 11:00:38 275.777,23.40,10.58,2017/10/16 12:00:38 110.481,22.26,17.16,2017/10/16 13:00:36 149.522,25.06,18.38,2017/10/16 14:00:36 82.292,17.59,8.26,2017/10/16 15:00:38 202.99,17.83,4.05,2017/10/16 16:00:38 102.178,17.50,4.52,2017/10/16 17:00:37 114.628,14.78,16.30,2017/10/16 18:00:38 102.745,12.89,13.11,2017/10/16 19:00:39 1795.938,11.67,1.76,2017/10/16 20:00:51 699.312,6.57,3.20,2017/10/16 21:00:52 352.963,3.47,0.98,2017/10/16 22:00:47 480.408,3.98,2.48,2017/10/16 23:00:47 ついでにグラフ化。 昼間は25Mbpsくらい出ているらしい。15時以降から下がりだして、21時から25時までは1桁というありさま。 speedtest-cli インストール スピードテストをCUIで行う場合は、恐らくspeedtest-cli一択。 pipで入れるので、Pythonのインストールから。 sudo apt install python 続いて pip install speedtest-cli 使い方 一番シンプルなのは speedtest これだけ。speedtest-cliでも同じ結果が得られる。 データ収集には不要な出力が多いのと、サーバーをping値で自動選択してしまうので、 以下のように使うことにする。 speedtest --server 6476 --simple シェルスクリプト 1ライン化 上のコマンドをベースに、cronで実行するシェルスクリプトを作成する。 1行にping, download, uploadをまとめたいのでtrでカンマと置換する。 また、速度以外は不要なのでawkを使って、以下のように書く。 /usr/local/bin/speedtest --server 6476 --simple | awk &#39;{print $2}&#39; | tr &#39;\n&#39; &#39;,&#39; &amp;gt;&amp;gt; ~/tmp/speedtest.log cronではコマンドを絶対パスで指定する必要があるので、絶対パスで書いている。 実行日時を追加 date &amp;quot;&#43;%Y/%m/%d %H:%M:%S&amp;quot; &amp;gt;&amp;gt; ~/tmp/speedtest.log リトライ機能を追加 ここまででほぼ完成だけど、結構な頻度で失敗してたのでリトライ機能を追加した。 cronでリトライしてくれればいいのに。 以下が完成形。 #!/bin/bash command=&amp;quot;/usr/local/bin/speedtest --server 6476 --simple&amp;quot; NEXT_WAIT_TIME=0 until RET=`$command` || [ $NEXT_WAIT_TIME -eq 4 ]; do sleep $(( (NEXT_WAIT_TIME&#43;&#43;) * 60 )) done echo &amp;quot;$RET&amp;quot; | awk &#39;{print $2}&#39; | tr &#39;\n&#39; &#39;,&#39; &amp;gt;&amp;gt; ~/tmp/speedtest.log date &amp;quot;&#43;%Y/%m/%d %H:%M:%S&amp;quot; &amp;gt;&amp;gt; ~/tmp/speedtest.log 4回までリトライする。リトライ間隔は、1分、2分、3分、4分と伸ばしていくようにした。 cron 上のシェルスクリプトをcronに登録する。 crontab -e すると、エディタが立ち上がる（もしくはエディタ選択画面になるので選ぶ）ので、 以下のように1時間毎に実行するように設定する。 0 * * * * /usr/local/bin/speedtest_cron.sh これで完成。 注意点 実行環境はRaspberry Piを想定しているが、場合によってはRaspberry Pi自体がボトルネックとなって、 正しい速度が取れないので注意。うちの場合はMac Book Airを有線でつないで測定してもだいたい同じような値が取れることを確認している。 関連ポスト マンション付属の光回線が早くなった話]"
}
,{
url: "http://shidetake.com/mysql_install/",
title: "Ubuntu 16.04にmysqlをインストールする",
content: "[aptで適当にインストールすれば使えると思っていたが、意外と手間取ったので、手順を書いておく。 install ここは簡単。 sudo apt install mysql-server mysql-client こんだけ。ただ、既に別のバージョンのものが入っていたりするので、キレイにしたい場合は、 先に以下のコマンドを送る。別バージョンのmysqlを削除することで、色々動かなくなる可能性もあるので注意。 sudo dpkg -l | grep mysql でインストール済みのmysqlを確認して、 sudo apt remove --purge mysql* こんな感じで、消していく。--purgeオプションは、関連する設定ファイルも一緒に削除するのもので、 この後の再インストール後に悪さしないためにも指定しておいたほうがよい。 rootパスワード設定 mysql -uroot で入ろうとしたら、ERROR 1698 (28000): Access denied for user &#39;root&#39;@&#39;localhost&#39;と怒られた。 初期パスワードは無かったと思ってたが、仕様が変わったらしい。 で色々やってみたんだけど、実はやっぱり初期パスワードは設定されていなくて、sudoつければいいだけだった。 ということで、改めて sudo mysql -uroot これでOK。]"
}
,{
url: "http://shidetake.com/dasher_systemd/",
title: "Raspberry PiでSystemdを使ってdasherをサービス化",
content: "[Amazon Dash Buttonを使ってLINEに通知する方法を以前書いた。 記事には記載していないが、このときは、init.dを使って自動起動するように設定していた。 最近はinit.dは古くて、Systemdが推奨されているようなので、これに書き換えることにした。 サンプルソース GitHubのWiki にサンプルソースがあったのでそのまま記載する。 [Unit] Description=Dasher After=network.target [Service] Type=simple #user with access to dasher files User=root WorkingDirectory=/home/pi/dasher #use command &amp;quot;which npm&amp;quot; to determine correct location of NPM ExecStart=/usr/local/bin/npm run start Restart=on-failure RestartSec=10 KillMode=process [Install] WantedBy=multi-user.target Unitセクション SystemdではUnitという単位で処理を管理する。 init.dで言うサービスに当たる概念のようだ。 ここではdasherが1つのUnitということになる。 Description Unitの説明を書く。 After このUnitが起動するタイミングを記述する。 ここではnetwork.targetの後に実行することになる。 .targetというのは今回のように順序関係や依存関係を定義する際に、複数のUnitをグループ化するために使われる。 network.targetの実体は、/lib/systemd/system/network.targetにあるが、内容を確かめずとも、ネットワーク関連のUnit群であることがわかる(のでこれ以上詳しくは見ない)。 ちなみにSystemdの起動時にはdefault.targetというUnitが有効化されることになっており、これに依存するUnitがまとめて有効化されていく。 Serviceセクション このセクションはUnitの種類によって異なる。 デーモンを起動する場合には.serviceなのでServiceセクションとなる。他にも上に出てきた.targetや.deviceなどいくつかの種類がある。 Type サービスプロセスの起動完了の判定方法。 simpleを指定すると、ExecStartで指定したコマンドを実行したタイミングで起動完了とみなす。 起動完了をどう使うかは未調査。上述のAfterなど順序関係のためだろうか。 User 実行ユーザを指定する。rootで実行することになる。 WorkingDirectory 実行時のカレントディレクトリを指定する。 ここではpiユーザーのhomeにdasherを取得したことになっている模様。 通常はセキュリティの観点からもpiユーザー以外にしているはずなので、ここは変更必須。 ExecStart サービスを起動するコマンド。フルパスじゃないとダメらしい。 このタイミングではまだパスが通ってないのだろうか？ Restart 再起動条件を指定する。on-failureは終了コード0以外で停止した場合に再起動する設定。 ようするに異常終了した時は再起動してねってこと。 RestartSec 再起動する前の待ち時間。Restartの設定と合わせて読むと、異常終了を検知したら10秒後に再起動してねってこと。 KillMode サービス停止時にプロセスが残っていた場合の処理を指定する。 processはメインプロセスが残っていたらkillする設定。 Installセクション systemctl enable/disableに関する設定群 WantedBy init.dで言うrunlevelの指定に相当する。 multi-user.targetはrunlevel 3に相当するので、CUIでログインすると起動することになる。 セットアップ 上の設定を /etc/systemd/system/dasher.serviceに置いて、 systemctl list-unit-files --type=service | grep dasher すると、dasher.serviceがサービスとして認識されていることがわかる。 この時点ではdisable状態。 続いて自動起動を有効化する。 sudo systemctl enable dasher してから、先程のコマンドを再送すると、enableに変わっているはず。 この状態で再起動する。そして、 sudo systemctl status dasher でactive状態になっていればOK]"
}
,{
url: "http://shidetake.com/ddns/",
title: "DDNSでLANの外から自宅のRaspberry PiにSSH接続する",
content: "[普通のインターネット回線では、グローバルIPが定期的に変わってしまうため、 自宅に置いたRaspberry Piなんかをサーバー代わりに使うには不便。 かといって、固定グローバルIPアドレスを貰うには月1000円くらい払う必要がある。 そこで、Dynamic DNS (DDNS) というサービスを利用することにする。 DDNSとは Domain Name System (DNS) は、ドメイン名とIPアドレスを紐付けるシステムのこと。 サーバーへのアクセスは通常、IPアドレスによって対象サーバーを判別するが、 IPアドレスは数値の羅列で人間には覚えにくいので、ドメイン名という覚えやすい名称に変換する。 Dynamic DNSというのは、その紐付けをDynamicに、つまり動的に行う。 グローバルIPが変化しても、ドメイン名との紐付けを再度行うことで、ドメイン名の固定ができるというわけ。 世の中にはこんな便利なサービスを無料で提供してくれる人たちがいる。 登録 いくつか選択肢はあるが、今回はno-ipというサービスを使うことにした。 なぜなら、raspberryPi にnoipでDDNSの設定をする方法というサイトを参考にしたため。 登録・設定から自動実行まで、ほぼほぼこの手順でいける。 ただ、Dynamic Domain Update Client (DUC) の自動起動の設定だけは、少し好みではなかったのでアレンジした。 DUCの自動起動設定 /etc/init.d/noip2を以下のように書き換えた。 自動起動の設定を先頭に追加しただけ。これにより、わざわざrc.localに起動のためのコマンドを書かなくとも、 自動で起動してくれる。 #! /bin/sh # /etc/init.d/noip2.sh ### BEGIN INIT INFO # Provides: noip2 # Required-Start: $all # Required-Stop: # Default-Start: 2 3 4 5 # Default-Stop: ### END INIT INFO # Supplied by no-ip.com # Modified for Debian GNU/Linux by Eivind L. Rygge &amp;lt;eivind@rygge.org&amp;gt; # corrected 1-17-2004 by Alex Docauer &amp;lt;alex@docauer.net&amp;gt; # . /etc/rc.d/init.d/functions # uncomment/modify for your killproc DAEMON=/usr/local/bin/noip2 NAME=noip2 test -x $DAEMON || exit 0 case &amp;quot;$1&amp;quot; in start) echo -n &amp;quot;Starting dynamic address update: &amp;quot; start-stop-daemon --start --exec $DAEMON echo &amp;quot;noip2.&amp;quot; ;; stop) echo -n &amp;quot;Shutting down dynamic address update:&amp;quot; start-stop-daemon --stop --oknodo --retry 30 --exec $DAEMON echo &amp;quot;noip2.&amp;quot; ;; restart) echo -n &amp;quot;Restarting dynamic address update: &amp;quot; start-stop-daemon --stop --oknodo --retry 30 --exec $DAEMON start-stop-daemon --start --exec $DAEMON echo &amp;quot;noip2.&amp;quot; ;; *) echo &amp;quot;Usage: $0 {start|stop|restart}&amp;quot; exit 1 esac exit 0 動作確認 自動起動するかどうか、実際に再起動して確かめる。 sudo reboot からの、 sudo /usr/local/bin/noip2 -S Process 445, started as noip2, (version 2.1.9) Using configuration from /usr/local/etc/no-ip2.conf Last IP Address set 0.0.0.0 Account hogehoge@gmail.com configured for: host hogehoge.ddns.net Updating every 30 minutes via /dev/wlan0 with NAT enabled. とすると、なんとIPアドレスが0になってしまっている。 失敗か？と思ったが、特に問題なくつながる。 30分後に再度確認したら正しいIPアドレスがセットされていた。どうやら起動後1発目のIPアドレスセットの内容は反映されないらしい。 Last IP Addressとあるので、初回より前のアドレスを出そうとしているのだろうか。]"
}
,{
url: "http://shidetake.com/stock_trendline_1/",
title: "株価チャートからトレンドラインを引く 1",
content: "[以前、Google Finance APIにより株価を取得する方法を書いた。 このデータからトレンドラインを引くスクリプトを作ろうと思う。 まずは、トレンドラインを引くためのポイントを選択する。 移動平均 取得した生データだと、エッジが立ちすぎていてトレンドラインのためのポイント選択には不向きなので、 とりあえず移動平均してみた。 require &#39;net/http&#39; require &#39;numo/gnuplot&#39; class GoogleFinance BASE_URL = &#39;https://finance.google.com/finance/getprices?&#39; def initialize(issue) @issue = issue end # 終値を取得する # param [String] 期間 # param [String] 間隔 [秒] def fetch_close(period = &#39;1Y&#39;, interval_sec = 86400) raw_data = Net::HTTP.get(URI.parse(BASE_URL &#43; &amp;quot;p=#{period}&amp;amp;i=#{interval_sec}&amp;amp;x=TYO&amp;amp;q=#{@issue}&amp;quot;)) @close = [] raw_data.lines do |line| @close &amp;lt;&amp;lt; line[/^[0-9]*,([0-9]*)/, 1] end @close.compact!.map!(&amp;amp;:to_i) end # 移動平均を求める # @param [Fixnum] 期間 # @note 終値が取得できている前提 def calc_move_average(term) # 移動平均のために端のデータを引き伸ばす tmp = @close[0..0] * (term / 2) &#43; @close &#43; @close[-1..-1] * (term / 2) @move_average = tmp.each_cons(term).map do |window| window.inject(:&#43;) / term.to_f end end end ISSUE_TOYOTA = 7203 # TOYOTAの証券コード MOVE_AVERAGE_TERM = 10 # 移動平均の期間 google_finance = GoogleFinance.new(ISSUE_TOYOTA) close = google_finance.fetch_close move_average_close = google_finance.calc_move_average(MOVE_AVERAGE_TERM) Numo.gnuplot do set title: &#39;TOYOTA&#39; plot [close, w: &#39;lines&#39;, t: &#39;close&#39;], [move_average_close, w: &#39;lines&#39;, t: &#39;move\_average&#39;] end こんな感じ。前回の記事からメソッド名など少し変えているので注意。 移動平均を求める方法はググったらいくらでも出てくるのであまり書かないが、 データの端の扱いだけは注意が必要。今回は端のデータを引き伸ばすことにした。 0で埋めるとか色々あるが、簡単にできて誤差が少ないのが特徴。 移動平均の期間をどう決めるかが難しいところだが、いったん10日とした。あとで調整する。 極値 移動平均から極値を求める。 今回は、となりの値との差分を計算して、その差分の正負が入れ替わる瞬間を極値とする。 ただ、激しく値動きしていると、極値が多くなりすぎるので、少し計算期間を長くすることでフィルタする。 例えば計算期間を5とすると、その差分が正 =&amp;gt; 正 =&amp;gt; 負 =&amp;gt; 負ならば極大値、負 =&amp;gt; 負 =&amp;gt; 正 =&amp;gt; 正ならば極小値。 この期間も移動平均の期間同様、どうやって決めるかが難しい。今回は7とした。 require &#39;net/http&#39; require &#39;numo/gnuplot&#39; class GoogleFinance # 省略 # 極値を求める # @param [Fixnum] 期間 大きいほど微小な変動を無視する # @note move_averageが求まっている前提 # @note 定めた期間の中央まで単調増加し、残りが単調減少していれば極小値とする # @note 定めた期間の中央まで単調減少し、残りが単調増加していれば極大値とする def calc_extremum(term) local_min = { x: [], y: [] } local_max = { x: [], y: [] } @move_average.each_cons(term).with_index do |window, i| mid_index = i &#43; term / 2 # windowの中央 diff = window.each_cons(2).map { |arr| arr[1] - arr[0] } if diff[0..(term / 2 - 1)].all? { |elm| elm &amp;lt; 0 } and diff[(term / 2)..-1].all? { |elm| 0 &amp;lt; elm } # 負 =&amp;gt; 正 local_min[:x] &amp;lt;&amp;lt; mid_index local_min[:y] &amp;lt;&amp;lt; @close[mid_index] elsif diff[0..(term / 2 - 1)].all? { |elm| 0 &amp;lt; elm } and diff[(term / 2)..-1].all? { |elm| elm &amp;lt; 0 } # 正 =&amp;gt; 負 local_max[:x] &amp;lt;&amp;lt; mid_index local_max[:y] &amp;lt;&amp;lt; @close[mid_index] end end return local_min, local_max end end ISSUE_TOYOTA = 7203 # TOYOTAの証券コード MOVE_AVERAGE_TERM = 10 # 移動平均の期間 LOCAL_SIZE = 7 # 極値計算の範囲 google_finance = GoogleFinance.new(ISSUE_TOYOTA) close = google_finance.fetch_close move_average_close = google_finance.calc_move_average(MOVE_AVERAGE_TERM) local_min, local_max = google_finance.calc_extremum(LOCAL_SIZE) Numo.gnuplot do set title: &#39;TOYOTA&#39; plot [close, w: &#39;lines&#39;, t: &#39;close&#39;], [move_average_close, w: &#39;lines&#39;, t: &#39;move\_average&#39;], [local_min[:x], local_min[:y], w: &#39;points&#39;, t: &#39;local\_min&#39;], [local_max[:x], local_max[:y], w: &#39;points&#39;, t: &#39;local\_max&#39;] end GoogleFinanceクラスはcalc_extremumメソッド以外は変わっていないので省略した。 少し見にくいが、緑点が極小値、紫点が極大値。それなりにいい具合の場所を選んでいるように思える。 実際に線を引くのは次回。]"
}
,{
url: "http://shidetake.com/google_finance_api/",
title: "Google Finance APIを使って株価チャートを作成",
content: "[株を買うかどうか判断するために株価チャートに補助線を書いたりする手法があるらしい。 手軽に補助線（トレンドラインと呼ぶらしい）を引いてくれるスクリプトでも作ろうかと思い、 まずは普通の株価チャートを描いてみた。 そのへんに落ちてるアプリでもできそうだけど、最終的に買いかどうか判断させたり、 色々自由にカスタマイズするには自分で作ったほうがいいかなと。 2017.9.16追記 これを公開した直後にAPIのURLが変わった模様。 元はhttps://www.google.com/finance/getprices?だったが、今はhttps://finance.google.com/finance/getprices?になっている。 この記事のコードも修正したので、そのまま使えるはず。 Google Finance API 国内で株価を取得するためのAPIは（正規には）ないらしい。 ただ、Google Finance APIという非公式のAPIがあり、使ってる人は使っているとか。 google financeのAPIのメモという記事が参考になる。 今回やりたいことのほとんどはここに書いてある。 違いは、 Rubyを使う (Pythonのが需要ありそうだけど) チャートを描く くらい。 まずは適当な銘柄の株価を以下のように取得する。 ここではトヨタにした（丸パクリ）。 require &#39;net/http&#39; class GoogleFinance BASE_URL = &#39;https://finance.google.com/finance/getprices?&#39; def initialize(issue) @issue = issue end def stock(period = &#39;1Y&#39;, interval_sec = 86400) raw_data = Net::HTTP.get(URI.parse(BASE_URL &#43; &amp;quot;p=#{period}&amp;amp;i=#{interval_sec}&amp;amp;x=TYO&amp;amp;q=#{@issue}&amp;quot;)) data = [] raw_data.lines do |line| data &amp;lt;&amp;lt; line[/^[0-9]*,([0-9]*)/, 1] end data.compact end end google_finance = GoogleFinance.new(7203) stock_data = google_finance.stock 取得したデータを1行ずつ取ってきて、/^[0-9]*,([0-9]*)/, 1という正規表現で終値を抜き出している。 先頭に数値があり、カンマ挟んで次の数値を取得するという意味。 これを配列に突っ込んで、先頭の方にあるゴミ（抽出条件などが書かれている）をcompactで取り除くことで、 この1年間のトヨタ株の終値の配列が完成。 チャート描画 これはgnuplotを使う。 Numo::Gnuplotというgemが便利そうだったので、これを使った。 gem install numo-gnuplot してから、 （当然、gnuplotも入れる） require &#39;net/http&#39; require &#39;numo/gnuplot&#39; class GoogleFinance # 省略 end google_finance = GoogleFinance.new(7203) stock_data = google_finance.stock Numo.gnuplot do set title:&#39;TOYOTA&#39; plot stock_data, w:&#39;lines&#39; end これだけ。GoogleFinanceクラスは変わらないので省略した。 得られたチャートを貼って終わり。]"
}
,{
url: "http://shidetake.com/iij_coupon/",
title: "IIJmioの追加クーポンをLINE botに買ってもらう",
content: "[IIJmioにはIIJmioクーポンスイッチというiPhoneアプリが用意されているが、 残りの通信量（IIJではクーポンと呼ぶ）を見ることはできても、追加購入することができない。 購入はブラウザから。 不便なのでLINE botに買ってもらうことにした。 実行画面 構成 LINEメッセージをMessaging APIが受けて、Google Apps Scriptを発動 GASはソケット通信でVPS上のrubyスクリプトを起こす rubyスクリプト上でCapybaraさんがIIJmioのウェブページにクーポン購入を依頼する クローラー作成 Kindleのハイライト通知のときと同じような作り。 動作環境も同じ。 # coding: utf-8 require &#39;capybara&#39; require &#39;capybara/dsl&#39; require &#39;capybara/poltergeist&#39; require &#39;selenium-webdriver&#39; class BuyIIJmioCoupon include Capybara::DSL SELENIUM = 0 POLTERGEIST = 1 USER_NAME = &#39;your_user_name&#39; PASSWORD = &#39;your_password&#39; def initialize(driver) Capybara.app_host = &#39;https://www.iijmio.jp/service/setup/hdd/charge/&#39; Capybara.default_max_wait_time = 5 case driver when SELENIUM Capybara.current_driver = :selenium Capybara.javascript_driver = :selenium Capybara.register_driver :selenium do |app| Capybara::Selenium::Driver.new(app, :browser =&amp;gt; :chrome) end when POLTERGEIST Capybara.current_driver = :poltergeist Capybara.javascript_driver = :poltergeist Capybara.register_driver :poltergeist do |app| Capybara::Poltergeist::Driver.new(app, {:timeout =&amp;gt; 120, js_errors: false}) end page.driver.headers = {&#39;User-Agent&#39; =&amp;gt; &#39;Mac Safari&#39;} end end def login visit(&#39;&#39;) fill_in &#39;j_username&#39;, :with =&amp;gt; USER_NAME fill_in &#39;j_password&#39;, :with =&amp;gt; PASSWORD click_button &#39;ログイン&#39; end def buy select &#39;1枚（100MB）&#39;, from: &#39;selectList&#39; click_button &#39;次へ&#39; check &#39;confirm&#39; click_button &#39;お申し込み&#39; end end crawler = BuyIIJmioCoupon.new(BuyIIJmioCoupon::POLTERGEIST) crawler.login crawler.buy 追加クーポン購入ページに直接アクセスして、ログインしたあとで、 購入枚数を選択して購入ボタンをポチポチ押していくだけ。購入枚数は1枚で固定した。 デバッグ用にSELENIUMで動作させることもできるようにしている。 VPSで待ち受ける これもKindleのハイライト通知と同じ。 上のソースを少し変えて、以下のようにする。 # coding: utf-8 require &#39;capybara&#39; require &#39;capybara/dsl&#39; require &#39;capybara/poltergeist&#39; require &#39;selenium-webdriver&#39; require &#39;socket&#39; class BuyIIJmioCoupon # 上と同じなので省略 end gs = TCPServer.open(23456) addr = gs.addr addr.shift printf(&amp;quot;server is on %s\n&amp;quot;, addr.join(&amp;quot;:&amp;quot;)) crawler = BuyIIJmioCoupon.new(BuyIIJmioCoupon::POLTERGEIST) loop do s = gs.accept print(s, &amp;quot; is accepted\n&amp;quot;) begin crawler.login crawler.buy rescue raise end print(s, &amp;quot; is gone\n&amp;quot;) s.close end Google Apps Script LINE Messaging APIから以下のスクリプトを実行するように紐付ければ (参考)、 ソケット通信でVPS上のスクリプトを起こせる。 こちらから送った文章にiijという文字列が含まれている場合にトリガがかかるようにした。 なお、冒頭の実行画面ではLINE botからACKが返っているが、以下のソースでは省略している。 var CHANNEL_ACCESS_TOKEN = &#39;your_line_channel_access_token&#39;; var SERVER_ADDRESS_IIJ = &#39;http://your_server_address.com:23456&#39; function doPost(e) { Logger.log(&#39;doPost&#39;) var events = JSON.parse(e.postData.contents).events; events.forEach (function(event) { if (event.type == &#39;message&#39;) { parseMessage(event); } }); } function parseMessage(e) { if (e.message.text.match(/iij/)) { buyIIJmioCoupon() } } function buyIIJmioCoupon() { UrlFetchApp.fetch(SERVER_ADDRESS_IIJ); } 以上。]"
}
,{
url: "http://shidetake.com/blog_img/",
title: "IFTTTとGoogle Photoを連携するとブログ用の画像準備がはかどる",
content: "[ブログに貼る画像置き場として、Google Photoを選んだ。 github-pageでホスティングしているのだが、画像をアップするのはイマイチかなと。 容量制限はないものの、1GBまでを推奨しているらしいし。 というわけで、Google Photoの画像を簡単に直接ブログに貼り付ける方法を模索していたが、 IFTTTを使って直リンクアドレスをメールしてもらうのが一番簡単だった。 設定手順 IFTTTでNew Appletボタンをクリック サービスはGoogle Photoを選択 トリガはNew photo uploaded 現時点ではこれしか選べない アルバムは適当なものを選ぶ 共有アルバムは選べないので注意 恐らくPicasa APIの仕様 アクションサービスはメールを選択 メールが嫌いならLINEでもよい アクションはSend me an email Subjectは適当に入れる 例: 画像URL {{CreatedAt}} {{CreatedAt}}はトリガがかかった日時 Bodyは{{PhotoUrl}} Create actionボタンを押して完了 実行手順 トリガ用のアルバムに写真を放り込む 1分程するとメールが届く 開いてURLをコピーしてブログに貼り付ける 特徴 大きい画像をいい具合に縮小してくれる 細かい条件は不明だが、いい具合に縮小してくれる PicasaのAPIで取得できるURLだとこうなってしまう模様 Picasa APIの使い方はこちら 短縮URLにして送ってくれる IFTTTの機能（設定が必要かも） オリジナルアドレスだと長過ぎてブログに貼り付けたくないので助かる]"
}
,{
url: "http://shidetake.com/icam/",
title: "iPadをウェブカメラ化してiCamSourceと連携",
content: "[iCamを使って外出先から部屋にいる犬の様子を見たいが、MBAのカメラ1つじゃ視野が狭い。 ということで、部屋に置いているiPadのカメラの映像も同時に見れるようにした。 使わなくなった古いiPhoneを使ってもよい。 必要なもの PC (今回はMBA) EpocCam Viewer Pro CamTwist iCamSource iPad EpocCam iPhone iCam 構成 iPadのウェブカメラ化 EpocCamというアプリを使って、iPadのカメラ画像をMBAで受信する。 これは特に設定もいらない簡単アプリ。同一LANである必要があるので、それだけ注意。 更に、それをCamTwistというアプリで受けることで、ウェブカメラの入力画像のように使えるようになる。 EpocCamが繋がっている状態 (上図左下) で、CamTwistの設定をSyphonにして、Syphon ServerとしてEpocCam Viewer Proを選択する (上図上) 。 CamTwistのPreviewにEpocCamと同じ映像が表示されれば成功 (上図右下) 。 iCamの設定 MBAにiCamSourceというアプリを入れて、VideoにCamTwist (2VUY)を選択する。 idとパスワードは適当に入力して、右下のStartボタンを押して、EpocCamと同じ映像が表示されれば成功。 iPhoneに入れたiCamアプリにidとパスワードを入力するとiPadのカメラの映像を見ることができる。]"
}
,{
url: "http://shidetake.com/wireless_tv/",
title: "テレビのアンテナ線を無線化する",
content: "[テレビ端子がイマイチな場所にあって、テレビを置く位置が限られてしまうので無線化を考えた。 マンションなのでアンテナ設置など大掛かりな方法は除いて、 ざっと調べたところ、以下の方法がある。 室内アンテナを設置する PS4とnasneを使う方法 Apple TVとピクセラのワイヤレステレビチューナーを使う方法 3つ目の案を採用することにした。 理由はApple TVを持っているから。 もし何も持っていなければ、PS4案にしたと思う。PS4の方が圧倒的にUIが洗練されているし、 何よりピクセラのアプリの評判の悪さがすごい（実際、使ってみてイマイチと感じる）。 それでも有線の頃に比べればかなり良くなった。 必要なもの Apple TV (第4世代) ピクセラのワイヤレステレビチューナー PIX-BR310L 無線LAN中継器 BUFFALO WEX-733D 無線LANルーター 親機 構成 環境構築手順 無線LAN環境を構築 11acで組んだほうが良い テレビ端子近くに無線LAN中継器を設置し、有線に変換 ピクセラチューナーをテレビ端子と有線LANに接続 Apple TVとテレビをHDMIケーブルで接続 Apple TVは当然無線LANに接続 Apple TVにピクセラのStation TVアプリをインストール アプリでテレビの映像が受信できれば成功 メリット／デメリット これで終わりだと味気ないのでこの構成のメリット／デメリットを少しだけ。 アンテナ線が不要なので配線がスッキリする これが一番のメリット。ただしPS4案や室内アンテナ案でも実現可能。 全てがApple TVで完結するので、入力切替が不要になる 我が家ではApple TVでHuluを観たり、NASに入れた動画を観たりとApple TVをヘビーに使っている。 有線のときは、テレビとApple TVの入力切替が必要で、結構煩わしかった。 Apple TVのスリープ時にテレビの電源を落とす設定と、Apple TVリモコンによる音量変更設定 (外部スピーカーの設定) をすると、テレビのリモコンも不要になる。 これが、PS4案よりも優れている唯一のポイントだと思う。 がんばれば外出先でスマホでテレビが見れる iPhoneアプリもあるので、家のLANに接続できれば外でも使える。 VPNを張る必要があるのでハードルは高め。いつか記事にするかも。 録画は諦めたほうがいい ピクセラチューナーに外付けHDDを接続すると録画もできるが、UIがクソなので使ってない。 録画予約もできない。Windowsのアプリなんかではできるのかもしれないが、よく知らない。 Mac用のアプリを2500円も出して買ったが起動すらしないクソアプリだったのでMacでは無理。 アプリの開発もたぶん止まっているので、今後の改善も期待できない。]"
}
,{
url: "http://shidetake.com/google_photo_api_2/",
title: "Picasa APIを使ってGoogle Photoからデジカメ写真だけを取得 2",
content: "[前回の続き。 今回は任意のアルバムから写真の情報を取得する。 写真情報の取得 your_album_nameアルバムから取得する場合は以下のような感じ。 require &#39;picasa&#39; client = Picasa::Client.new( user_id: &#39;your@email&#39;, access_token: &#39;your_access_token&#39; ) albums = client.album.list.entries album = albums.find { |a| a.title == &#39;your_album_name&#39; } photos = client.album.show(album.id).entries photos.each do |photo| puts &amp;quot;#{photo.id}, #{photo.exif.time}, #{photo.title}, #{photo.exif.model}&amp;quot; end Exif情報から撮影機器のモデル名を取得することもできた。 これで最初にやりたかったことはほぼできたが、 このスクリプトだと1000枚しか処理できないという欠点があるのでもう少し続ける。 大量の写真情報の取得 start-indexオプションを使って以下のように書くことで、1000枚以上の写真を処理することができる。 ただし、start-indexの最大値は10001らしいので、この方法でも11000枚までしか処理できない。 11000枚で十分事足りたので、この対処法までは調べていない。 PAGE_NUM_MAX = 11 page_num = [album.numphotos / 1000, PAGE_NUM_MAX].min page_num.times do |page| opt = {} opt[&#39;start-index&#39;] = page * 1000 &#43; 1 photos = client.album.show(album.id, opt).entries photos.each do |photo| puts &amp;quot;#{photo.id}, #{photo.exif.time}, #{photo.title}, #{photo.exif.model}&amp;quot; end end albumの指定まではこれまでと変わらないので省略した。]"
}
,{
url: "http://shidetake.com/google_photo_api_1/",
title: "Picasa APIを使ってGoogle Photoからデジカメ写真だけを取得 1",
content: "[Google Photoの検索では、Exif情報の撮影機器のモデル名をキーにすることができないらしい。 あるデジカメで撮った写真だけを抽出するために試行錯誤した結果、 PicasaのAPIを使うことで実現できたので、そのやり方を記す。 ちなみにGoogle PhotoのAPIはまだ用意されておらず、Piacasa APIがその代替となっている。 Picasa APIを使う Access tokenの取得 Gmailのときとは違い、PicasaのAPIを使うのは少しコツがいる。 既にPicasaのサービスが終了しているためか、API MANAGERで検索してもPicasa APIが出てこない。 そこで、Access tokenを取得するためにOAuth 2.0 Playgroundを使う。 Select the scopeの一覧から、Picasa Web v2を選択して表示されるURLをクリックする。 Authorize APIsボタンが有効になるので、それを押してAPIを有効にする許可を与える。 Authorization codeが取得できるので、続けてExchange authorization code for tokensボタンを押下。 これでAccess tokenが得られる。 Access tokenの確認 得られたAccess tokenの確認も兼ねて、簡単なAPIを使ってみる。 まずはRubyでPicasa APIを簡単に使えるようにするgemをインストール。 gem install picasa あとは以下のようなスクリプトでアルバム名の一覧を取得する。 require &#39;picasa&#39; client = Picasa::Client.new( user_id: &#39;your@email&#39;, access_token: &#39;your_access_token&#39; ) albums = client.album.list.entries albums.each { |a| p a.title } your@emailとyour_access_tokenにはそれぞれ自分の（Access token取得時に許可を与えた）アカウントと、上で取得したAccess tokenを入れる。 Google Photoのアルバム名の一覧が表示されれば成功。Access tokenがちゃんと使えることがわかった。 次回以降で写真の取得、Exif情報の取得あたりのやり方を書いていく。]"
}
,{
url: "http://shidetake.com/ripping_3/",
title: "HandBrakeCLIによるDVDリッピング 3",
content: "[前回からの続き。 これまではisoファイルからリッピングする形だったが、DVDドライブから直接リッピングする。 DVDドライブのパスを取得 diskutilコマンドを使う（たぶんmacOSでしか使えない）。 DVDをドライブに入れた状態で、 diskutil list すると、HDD含め、OSが認識しているディスクのパーティション一覧が出てくる。 NAMEの部分に入れたDVDの名前が表示されているのがあるはず。 例えば以下のような感じ。 /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: VIBY_521 *2.0 GB disk2 この場合、/dev/disk2を入力として指定することでDVDから直接リッピングが可能になる。 HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c 1 -i /dev/disk2 -o hoge_1.mkv 前回の最後のスクリプトと組み合わせて、 chapter_num=`lsdvd /dev/disk2 | grep Chapters | awk &#39;{gsub(/,/,&amp;quot;&amp;quot;); print $6}&#39;` for ((i = 1; i &amp;lt;= $chapter_num; i&#43;&#43;)); do HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c $i -i /dev/disk2 -o hoge_$(printf %02d $i).mkv done ここまでで、かなり形になったので今回でラストとする。 ただ、実はこれだけだと使えないタイプのDVDがある。 そのあたりは、そのうち載せようと思う。スクリプトを少し整理してGitHubで公開する予定。 お約束 違法なリッピングを推奨しているわけではないので注意。 コピーガードされたDVDをリッピングするのはダメ。ぜったい。]"
}
,{
url: "http://shidetake.com/ripping_2/",
title: "HandBrakeCLIによるDVDリッピング 2",
content: "[前回からの続き。 チャプター分割 チャプター毎にファイルを分割してリッピングしたい場合は-cオプションを使って HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c 1 -i hoge.iso -o hoge_1.mkv HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c 2 -i hoge.iso -o hoge_2.mkv HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c 3 -i hoge.iso -o hoge_3.mkv いちいちチャプター数だけ繰り返すのはバカらしいので、 for文を使って以下のように書く。 for ((i = 1; i &amp;lt;= 3; i&#43;&#43;)); do HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c $i -i hoge.iso -o hoge_$(printf %02d $i).mkv done 3の部分を変数にして、引数として渡してやれば、任意のチャプター数に対応できる。 チャプター数の取得 いちいち、チャプター数を指定するのは面倒なので、 lsdvdを使ってチャプター数を取得する。 brew install lsdvd からの lsdvd hoge.iso これでチャプター数を含む標準出力が得られる。 あとは適当に文字列を抜き出して使えば良い。 以下にawkを使った例を示す。 lsdvd hoge.iso | grep Chapters | awk &#39;{gsub(/,/,&amp;quot;&amp;quot;); print $6}&#39; 先ほどのfor文と組み合わせて chapter_num=`lsdvd hoge.iso | grep Chapters | awk &#39;{gsub(/,/,&amp;quot;&amp;quot;); print $6}&#39;` for ((i = 1; i &amp;lt;= $chapter_num; i&#43;&#43;)); do HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -c $i -i hoge.iso -o hoge_$(printf %02d $i).mkv done 長くなってきたので、いったんここまで。 次回はisoファイルではなく、DVDドライブから直接mkvに変換する方法。 お約束 違法なリッピングを推奨しているわけではないので注意。 コピーガードされたDVDをリッピングするのはダメ。ぜったい。]"
}
,{
url: "http://shidetake.com/ripping_1/",
title: "HandBrakeCLIによるDVDリッピング 1",
content: "[DVDのリッピングというのは、設定が異常に多い。 まず大きいところで、コーデック。汎用性の高いH.264にするのか、先を見越してH.265にするのか。 コンテナはどうする？mp4なんかはよく見るし汎用性ありそう。でも字幕や音声を切り替えるならmkvを選んだ方がいい。 フレームレートは？ノイズ除去フィルタの種類や強さも決めなきゃ。 と言った具合に無数に決めることがあり、それをいちいち設定してたら何十枚もあるDVDをリッピングするのにどれだけ時間がかかるかわからない。 しかも、今回設定した内容を次に覚えてないとまたノイズフィルタのかかり具合を見極める作業を繰り返すことになる。 そういった面倒なことを避けるために、DVDをドライブに挿入したら、あとはコマンドを一発送るだけでいつもの設定でリッピングできるようにした。 HandBrakeCLI 今回使ったのは、有名なリッピングソフトであるHandBrakeのCUIバージョンであるHandBrakeCLI。 Homebrewにはなかったので公式サイトからダウンロードしてインストールする。 ISOイメージからの変換 まずはISOイメージから適当な設定で変換する。 HandBrakeCLI -i hoge.iso -o hoge.mkv こんな感じ。デフォルトのオプションがどうなってるかはhelpに書いてあるとは思うが長すぎて読んでられない。 もう少しちゃんとした変換 完全にマニュアルでオプションを設定するより、プリセットオプションと組み合わせたほうがよい。 以下のコマンドで、プリセットオプションの一覧が出てくる。 HandBrakeCLI -z 画質重視＋音声や字幕の切り替えがしたいのでmkv形式のH.265 MKV 1080p30にしてみる。 HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; -i hoge.iso -o hoge.mkv せっかく音声と字幕の切り替えができるmkv形式にしたのに、このままでは1種類しか取り込んでくれないので、 --all-audioオプションと-sオプションを使って HandBrakeCLI -Z &#39;H.265 MKV 1080p30&#39; --all-audio -s &#39;1,2,3,4,5,6&#39; -i hoge.iso -o hoge.mkv こんな感じ。 DVDからの取り込みやチャプター分割など、もう少し突っ込んだ内容は次回以降。 お約束 違法なリッピングを推奨しているわけではないので注意。 コピーガードされたDVDをリッピングするのはダメ。ぜったい。]"
}
,{
url: "http://shidetake.com/gmail_api_2/",
title: "RubyでGmail本文を取得する 2",
content: "[前回はアクセストークンを取得して、メール本文を取得することに成功した。 ただしアクセストークンには期限があり、取得しなおさなくてはならない。 今回はリフレッシュトークンを使うことで、これを解決する。 リフレッシュトークンの取得 前回のアクセストークン取得の項に記載の通り。 リフレッシュトークンを使ったメールの取得 前回からの差分 require &#39;google/api_client&#39; require &#39;json&#39; -ACCESS_TOKEN = &#39;YourAccessToken&#39; &#43;CLIENT_ID = &#39;YourClientID&#39; &#43;CLIENT_SECRET = &#39;YourClientSecret&#39; &#43;REFRESH_TOKEN = &#39;YourRefreshToken&#39; APPLICATION_NAME = &#39;YourApplicationName&#39; # APIクライアントの準備 client = Google::APIClient.new(application_name: APPLICATION_NAME) -client.authorization.access_token = ACCESS_TOKEN &#43;client.authorization = Signet::OAuth2::Client.new( &#43; token_credential_uri: &#39;https://accounts.google.com/o/oauth2/token&#39;, &#43; audience: &#39;https://accounts.google.com/o/oauth2/token&#39;, &#43; scope: [&#39;https://www.googleapis.com/auth/drive.file&#39;], &#43; client_id: CLIENT_ID, &#43; client_secret: CLIENT_SECRET, &#43; refresh_token: REFRESH_TOKEN, &#43;) &#43; &#43;client.authorization.refresh! gmail = client.discovered_api(&#39;gmail&#39;) # query res = client.execute( api_method: gmail.users.messages.list, parameters: {&#39;userId&#39; =&amp;gt; &#39;me&#39;, &#39;q&#39;=&amp;gt;&#39;from:auto-confirm@amazon.co.jp&#39;}, ) # parse if res.status == 200 json = JSON.parse(res.body) json[&#39;messages&#39;].each do |mail_ids| mail = client.execute( api_method: gmail.users.messages.get, parameters: {&#39;userId&#39; =&amp;gt; &#39;me&#39;, &#39;id&#39;=&amp;gt;mail_ids[&#39;id&#39;]}, ) p mail.body end else puts &#39;error&#39; end これでこのシリーズは終わり。]"
}
,{
url: "http://shidetake.com/gmail_api_1/",
title: "RubyでGmail本文を取得する 1",
content: "[GmailのAPIをRubyで使う方法を何回かに分けて書く。 API準備 まずはAPIを使うための準備。https://console.developers.google.com/ にアクセスして、プロジェクトを作成する。 それから、Gmail APIを有効にし、認証情報を追加する。 使用するAPI =&amp;gt; Gmail API APIを呼び出す場所 =&amp;gt; その他のUI アクセスするデータの種類 =&amp;gt; ユーザーデータ あとは適当に選んで進めると、クライントIDとクライアントシークレットが取得できる。 Access Tokenの取得 取得したクライアントIDとクライアントシークレットから、 以下のスクリプトでアクセストークンとリフレッシュトークンを取得できる。 require &#39;net/http&#39; require &#39;uri&#39; require &#39;oauth2&#39; require &#39;launchy&#39; CLIENT_ID = &#39;YourClientID&#39; CLIENT_SECRET = &#39;YourClientSecret&#39; client = OAuth2::Client.new( CLIENT_ID, CLIENT_SECRET, :site =&amp;gt; &amp;quot;https://accounts.google.com&amp;quot;, :token_url =&amp;gt; &amp;quot;/o/oauth2/token&amp;quot;, :authorize_url =&amp;gt; &amp;quot;/o/oauth2/auth&amp;quot;) auth_url = client.auth_code.authorize_url( :redirect_uri =&amp;gt; &#39;urn:ietf:wg:oauth:2.0:oob&#39;, :scope =&amp;gt; &#39;https://www.googleapis.com/auth/gmail.readonly&#39;) # 表示されるURLをブラウザで開く Launchy.open auth_url print &amp;quot;authorization code:&amp;quot; authorization_code = gets.chomp res = Net::HTTP.post_form(URI.parse(&#39;https://accounts.google.com/o/oauth2/token&#39;), {&#39;client_id&#39; =&amp;gt; CLIENT_ID, &#39;client_secret&#39; =&amp;gt; CLIENT_SECRET, &#39;redirect_uri&#39; =&amp;gt; &#39;urn:ietf:wg:oauth:2.0:oob&#39;, &#39;grant_type&#39; =&amp;gt; &#39;authorization_code&#39;, &#39;code&#39; =&amp;gt; authorization_code}) puts res.body メールの取得 取得したアクセストークンを以下のように使って、Gmail APIが使える。 以下は、auto-confirm@amazon.co.jpからの受信メールを取得するスクリプト。 require &#39;google/api_client&#39; require &#39;json&#39; ACCESS_TOKEN = &#39;YourAccessToken&#39; APPLICATION_NAME = &#39;YourApplicationName&#39; # APIクライアントの準備 client = Google::APIClient.new(application_name: APPLICATION_NAME) client.authorization.access_token = ACCESS_TOKEN gmail = client.discovered_api(&#39;gmail&#39;) # query res = client.execute( api_method: gmail.users.messages.list, parameters: {&#39;userId&#39; =&amp;gt; &#39;me&#39;, &#39;q&#39;=&amp;gt;&#39;from:auto-confirm@amazon.co.jp&#39;}, ) # parse if res.status == 200 json = JSON.parse(res.body) json[&#39;messages&#39;].each do |mail_ids| mail = client.execute( api_method: gmail.users.messages.get, parameters: {&#39;userId&#39; =&amp;gt; &#39;me&#39;, &#39;id&#39;=&amp;gt;mail_ids[&#39;id&#39;]}, ) p mail.body end else puts &#39;error&#39; end 今後 アクセストークンには期限があり、一定時間経過すると使えなくなってしまう。 これを解決するのがリフレッシュトークンを使った方法。次回はこれを使っていつでもAPIを利用できるようにする。]"
}
,{
url: "http://shidetake.com/line_highlights_4/",
title: "KindleのハイライトをLINEに通知する 4",
content: "[前回はLINEのBOTにメッセージを送ることで、 ハイライトを取得して返信してくれるようにした。 現状では、設定したページ数だけスクレイピングして、全てのハイライトを返信することになっているので、 何度も同じハイライトが送られてきてしまう。 今回は最新のハイライトだけを返信するようにする。 今回でこのシリーズは最後。 方針 今まで取得したハイライトを記録しておいて、 差分だけを返信するという単純な仕様にした。 記録する方法は、これまた単純に外部ファイルに保存するだけ。 容量が増えてきたら、ソートしておく、データベースに記録する、などの工夫が必要になるかもしれないが、 とりあえず問題になるまでは単純な実装にする。 JSON形式で保存 以下のメソッドで@highlightsという配列をJSON形式にして保存する。 &#39;json&#39;を忘れずにrequireすること。 # ハイライトをJSON形式にして外部ファイルに保存する def store_highlights File.open(JSON_FILE_NAME, &#39;w&#39;) do |file| JSON.dump(@highlights, file) end end JSON形式のファイルを読み出し 以下のメソッドでJSON_FILE_NAMEというJSON形式ファイルから@highlights配列にデータを読み出す。 # 外部ファイルから既に取得しているハイライトを読み出す def restore_highlights return unless File.exist?(JSON_FILE_NAME) File.open(JSON_FILE_NAME, &#39;r&#39;) do |file| @highlights = JSON.load(file) end end あとは、@highlightsに存在しないハイライトだけをLINEで送信し、外部ファイルに保存すればよい。 全ソース githubに全ソースをあげた。 https://github.com/shidetake/line_kindle_highlights]"
}
,{
url: "http://shidetake.com/line_highlights_3/",
title: "KindleのハイライトをLINEに通知する 3",
content: "[前回はサーバで起動してLINE通知するところまで。 このままだとサーバ側で手動で起動しないといけない。これを解決する。 起動タイミング 仕様 cronで周期的に実行する仕様や、手動で合図してから一定時間だけ連続起動する仕様など、いくつか考えたが、 結局は手動で合図したタイミングで1度だけ起動する仕様にした。 LINEでタイミングを通知する方法であれば、手動でも大した手間ではない。 妥協案とも言える仕様だが、この仕様にした一番の理由は、 頻繁にアクセスすると、ロボットと疑われてCAPTCHAでブロックされてしまうため。 これを解除することもできるとは思うが、一気に難易度が上がるので今回は見送ることにした。 流れ 以下のような流れで起動要求を伝達する。 LINEで起動要求 ↓ LINEからGoogle Apps Scriptのスクリプト実行 ↓ ソケット通信でスクレイピング開始要求 実装 まずは前回まで作ったクローラーを、ソケット通信をトリガにして動作するようにする。 &#43;require &#39;socket&#39; &#43;gs = TCPServer.open(12345) &#43;addr = gs.addr &#43;addr.shift &#43;printf(&amp;quot;server is on %s\n&amp;quot;, addr.join(&amp;quot;:&amp;quot;)) crawler = LineKindleHighlights.new &#43;loop do &#43; s = gs.accept &#43; print(s, &amp;quot; is accepted\n&amp;quot;) &#43; crawler.scrape &#43; &#43; print(s, &amp;quot; is gone\n&amp;quot;) &#43; s.close &#43;end これでポート12345番でアクセスされると1度だけ動作するようになる。 つづいて、LINEをトリガにして動作するスクリプトをGoogle Apps Scriptとして作成する。 function doPost(e) { Logger.log(&#39;doPost&#39;) var events = JSON.parse(e.postData.contents).events; events.forEach (function(event) { if (event.type == &amp;quot;message&amp;quot;) { UrlFetchApp.fetch(&amp;quot;http://your_server_address.com:12345&amp;quot;); } }); } こんな感じでウェブアプリケーションとして導入して、LINE側のWebhookアドレスに紐付けると、 BOTに話しかけたタイミングでソケット通信が飛ぶようになる。 続きはこちら]"
}
,{
url: "http://shidetake.com/line_highlights_2/",
title: "KindleのハイライトをLINEに通知する 2",
content: "[前回はクローラーを作成してハイライトを取得するところまでだった。 今回はサーバで起動してLINE通知するところまでを作る。 サーバで起動 前回はSeleniumを使ったが、サーバで実行する場合はGUIアプリケーションにはできないので、 POLTERGEISTを使う。 def initialize - Capybara.current_driver = :selenium &#43; Capybara.current_driver = :poltergeist - Capybara.javascript_driver = :selenium &#43; Capybara.javascript_driver = :poltergeist Capybara.app_host = &#39;https://kindle.amazon.co.jp&#39; Capybara.default_max_wait_time = 5 - Capybara.register_driver :selenium do |app| &#43; Capybara.register_driver :poltergeist do |app| - # 最新のSeleniumではFirefoxが動作しない問題があるのでchromeを使う - Capybara::Selenium::Driver.new(app, :browser =&amp;gt; :chrome) &#43; Capybara::Poltergeist::Driver.new(app, {:timeout =&amp;gt; 120, js_errors: false}) end &#43; page.driver.headers = {&#39;User-Agent&#39; =&amp;gt; &#39;Mac Safari&#39;} end Amazonのウェブサイト固有の問題で、ユーザーエージェントの変更が必要なので、Mac Safariとしている。 これでサーバ（今回はUbuntuを使った）で動作するようになった。 LINE通知 line-bot-apiを使った。だいたい以下のような感じ。Channel Secret, Channel Access Token, userIdはLINE developersから取得して入力すること。 push_highlightメソッドを使って、前回標準出力していた文字列をLINEに送れる。 def initialize(driver) Capybara.current_driver = :poltergeist Capybara.javascript_driver = :poltergeist Capybara.app_host = &#39;https://kindle.amazon.co.jp&#39; Capybara.default_max_wait_time = 5 Capybara.register_driver :poltergeist do |app| Capybara::Poltergeist::Driver.new(app, {:timeout =&amp;gt; 120, js_errors: false}) end page.driver.headers = {&#39;User-Agent&#39; =&amp;gt; &#39;Mac Safari&#39;} if driver == POLTERGEIST &#43; @line = Line::Bot::Client.new do |config| &#43; config.channel_secret = &#39;&#39; &#43; config.channel_token = &#39;&#39; &#43; end &#43; @user_id = &#39;&#39; end &#43; def push_highlight(highlight) &#43; message = { &#43; type: &#39;text&#39;, &#43; text: highlight &#43; } &#43; @line.push_message(@user_id, message) &#43; end 続きはこちら]"
}
,{
url: "http://shidetake.com/line_highlights_1/",
title: "KindleのハイライトをLINEに通知する 1",
content: "[読書メモのためにハイライトした文章をコピペできる形にしたい。 KindleにはハイライトをメールやTwitterでシェアする機能があるが、これはアクションが増えて読書を妨げるので使いたくない。 ハイライトした内容は https://kindle.amazon.co.jp/your_highlights で見えるので、これを取得して通知するクローラーを作ればよい。 クローラー作成 RubyのCapybaraを使った。 動作環境 ruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-darwin15] selenium-webdriver (2.53.0, 2.52.0) capybara (2.14.0) ソースコード # coding: utf-8 require &#39;capybara&#39; require &#39;capybara/dsl&#39; require &#39;capybara/poltergeist&#39; require &#39;selenium-webdriver&#39; class LineKindleHighlights include Capybara::DSL KINDLE_EMAIL = &#39;your@email&#39; KINDLE_PASSWORD = &#39;your_password&#39; CRAWL_PAGE_NUM = 2 def initialize Capybara.current_driver = :selenium Capybara.javascript_driver = :selenium Capybara.app_host = &#39;https://kindle.amazon.co.jp&#39; Capybara.default_max_wait_time = 5 Capybara.register_driver :selenium do |app| # 最新のSeleniumではFirefoxが動作しない問題があるのでchromeを使う Capybara::Selenium::Driver.new(app, :browser =&amp;gt; :chrome) end end def scrape login # ページ読み込み待ち sleep 5 go_to_highlights CRAWL_PAGE_NUM.times do all(&#39;.title&#39;).each do |element| p element.text end all(&#39;.highlight&#39;).each do |element| p element.text end next_page end end private # Kindleのマイページにアクセスしログインする def login visit(&#39;&#39;) click_link &#39;sign in&#39; fill_in &#39;ap_email&#39;, :with =&amp;gt; KINDLE_EMAIL fill_in &#39;password&#39;, :with =&amp;gt; KINDLE_PASSWORD click_on &#39;signInSubmit&#39; end # Your Hightlightsのページに遷移する # @note ログイン直後のページで使う def go_to_highlights click_link &#39;Your Highlights&#39; end # 次のページに遷移 # @note Your Hightlightsのページで使うことで次の本のハイライトページに遷移する def next_page visit(find_by_id(&#39;nextBookLink&#39;, visible: false)[:href]) end end crawler = LineKindleHighlights.new crawler.scrape KINDLE_EMAILとKINDLE_PASSWORDだけ設定すれば動く CRAWL_PAGE_NUMは実質的には取得するハイライトの本の数になる (1ページに1冊分の情報が出る） 次のページへの遷移で少しハマった。hiddenなクラスのリンクを取得するにはvisible: falseする必要があるらしい とりあえず今回はここまで。 続きはこちら TODO クローラー作成 LINE通知機能 通知済みハイライトを通知しない サーバ上で動作]"
}
,{
url: "http://shidetake.com/time_capsule/",
title: "Time Machineの容量制限",
content: "[Time Capsuleをバックアップ用途だけでなく、NASとしても使いたい。 Time Machineを使うと、容量がいっぱいになるまでバックアップを取り続けてしまうので、 何らかの方法で容量を制限する必要がある。 パーティションを切って、バックアップ領域とデータ領域に分けるという方法が主流のようだ。 ただ、裏ワザ的な使い方のためあまり推奨しないという意見も多い。 そこで、別の方法を採用した。 ダミーファイルを置く ダミーファイルを置くことで、パーティションを切ることなく、Time Machineの侵食を逃れる。 新しくデータを置きたい場合は、ダミーファイルの一部をデータと置き換えればよい。 1GBのダミーファイルを100個と、10GBのダミーファイルを10個用意して、 計200GB分のデータ領域を確保することにした。 ダミーファイルの作成は以下のコマンドを使った。 dd if=/dev/zero of=./dummy_10GB bs=1000000 count=10000 実際にはfor文で以下のように使う。 ネットワーク越しなのでかなり時間がかかることに注意。 cd /Volumes/TimeCapsule for ((i=0; i &amp;lt; 100; i&#43;&#43;)); do dd if=/dev/zero of=./dummy_1GB_$i bs=1000000 count=1000; done for ((i=0; i &amp;lt; 10; i&#43;&#43;)); do dd if=/dev/zero of=./dummy_10GB_$i bs=1000000 count=10000; done]"
}
,{
url: "http://shidetake.com/dash_button/",
title: "Amazon Dash ButtonからLINE通知",
content: "[dasher導入 他の人が詳しく書いているので割愛。 個人的にはAmazon Dash Button と slackを連携させるという記事がとても参考になった。 LINEへのPUSH通知設定 LINE側の設定はLINE BOTでPUSH通知するを参照。 CHANNEL_ACCESS_TOKENとUSERIDは上記事で取得したもの。 MAC_ADDRESSはdasher導入時に取得したものを書くこと。 {&amp;quot;buttons&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;joy&amp;quot;, &amp;quot;address&amp;quot;: &amp;quot;MAC_ADDRESS&amp;quot;, &amp;quot;url&amp;quot;: &amp;quot;https://api.line.me/v2/bot/message/push&amp;quot;, &amp;quot;method&amp;quot;: &amp;quot;POST&amp;quot;, &amp;quot;headers&amp;quot;: { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer {CHANNEL_ACCESS_TOKEN}&amp;quot;}, &amp;quot;json&amp;quot;: true, &amp;quot;body&amp;quot;: { &amp;quot;to&amp;quot;: &amp;quot;USERID&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;JOY!&amp;quot; } ] } } ]}]"
}
,{
url: "http://shidetake.com/line_push/",
title: "LINE BOTでPUSH通知する",
content: "[登録 PUSH通知する場合はDeveloper Trialで登録 Google Apps ScriptでユーザID取得 友達に追加したユーザのIDを登録したメールアドレスにメールする 以下のコードでウェブアプリケーションとして導入する CHANNEL_ACCESS_TOKENはLINE depelopersで取得できる example@gmail.comは任意のgmailアドレス アプリケーションにアクセスできるユーザーは全員（匿名ユーザーを含む） var CHANNEL_ACCESS_TOKEN = &#39;CHANNEL_ACCESS_TOKEN&#39;; function doPost(e) { Logger.log(&#39;doPost&#39;) var events = JSON.parse(e.postData.contents).events; events.forEach (function(event) { if (event.type == &amp;quot;follow&amp;quot;) { mailUserId(event); } }); } function mailUserId(e) { MailApp.sendEmail(&#39;example@gmail.com&#39;, &#39;mailId&#39;, e.source.userId); } curlでMessaging APIのPUSH通知を使う CHANNEL_ACCESS_TOKENは上で使ったものと同じ USERIDはプッシュ先 今回は上で取得した自分のユーザIDを使う curl -X POST \ -H &#39;Content-Type:application/json&#39; \ -H &#39;Authorization: Bearer {CHANNEL_ACCESS_TOKEN}&#39; \ -d &#39;{ &amp;quot;to&amp;quot;: &amp;quot;USERID&amp;quot;, &amp;quot;messages&amp;quot;:[ { &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;Hello, world!&amp;quot; } ] }&#39; https://api.line.me/v2/bot/message/push 関連ポスト Amazon Dash ButtonからLINE通知]"
}
,{
url: "http://shidetake.com/post/",
title: "Posts",
content: "[]"
}]
